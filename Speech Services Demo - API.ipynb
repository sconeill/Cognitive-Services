{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Cognitive Services Speech API\n\n## Batch Transcription\n\nThis Notebook uses the REST API for Batch transcription of audio files from an Azure Blob Storage Container.\n\n## Prerequisites\n\n- Data must be uploaded into an Azure Blob Storage Container\n- Audio files must be in either WAV or MP3 format (with PCM Codec) or OGG (OPUS Codec), with a bitrate of 16-bit and a sample rate of either 8 or 16 kHz, in either mono or stero.\n- A [Cognitive Services Speech Subscription](https://ms.portal.azure.com/#blade/Microsoft_Azure_Marketplace/GalleryFeaturedMenuItemBlade/selectedMenuItemId/home/searchQuery/speech/resetMenuId/) will be required, using a **standard** tier subscription\n\nFurther details are available from the documentation here - https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/batch-transcription#prerequisites\n\nA Swagger is available for API testing and documentation here (see \"Custom Speech Transcriptions\" section) - https://uksouth.cris.ai/docs/v2.0/swagger\n\n## Process"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import modules\n\nimport requests\nimport json\nimport pandas as pd\n\n# Replace <Region> to match the region in which you've provisioned your Speech service.\nregion = \"uksouth\"\n\nrequest_url = \"https://\" + region + \".cris.ai/api/speechtotext/v2.0/transcriptions\"\n\n# Replace <Subscription Key> with your valid subscription key.\nsubscription_key = \"<Subscription Key>\"\n\n# Replace <blob URL> with the link to your Azure Blob Storage container\nblob_url = \"<blob URL>\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Access Token\n\nAn access token is needed to interact with the Speech API."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def get_token(subscription_key):\n    \n    fetch_token_url = 'https://' + region + '.api.cognitive.microsoft.com/sts/v1.0/issueToken?scope=speechservicesmanagement'\n    headers = {\n        'Ocp-Apim-Subscription-Key': subscription_key\n    }\n    response = requests.post(fetch_token_url, headers=headers)\n    access_token = str(response.text)\n    # print(access_token)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Job Parameters\nGive your transcription job a name and description, and provide details of any custom acoustic or language models you wish to use."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "parameters = {\n    \"recordingsUrl\": blob_url,\n    \"models\": [],\n    \"locale\": \"en-GB\",\n    \"name\": \"Sample Transcription\",\n    \"description\": \"Batch Audio Transcription Test\",\n    \"properties\": {\n        \"ProfanityFilterMode\": \"Masked\",\n        \"PunctuationMode\": \"DictatedAndAutomatic\",\n        \"AddWordLevelTimestamps\": \"True\"\n    }\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Create a Transcription Job"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "req = requests.post(request_url, headers = {'Content-Type': 'application/json', 'Ocp-Apim-Subscription-Key': subscription_key, 'Authorization': get_token(subscription_key)}, data = json.dumps(parameters))\n\nif req.status_code == 202:\n    print(\"Job successfully submitted\")\nelse:\n    print(\"Job Failed - \" + req.status_code + \". Refer to Swagger for error code documentation\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Get List of Transcription Jobs and Job Status\n\nOnce submitted, you will need to check the progress of the transcription."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "req = requests.get(request_url, headers = {'Ocp-Apim-Subscription-Key': subscription_key, 'Authorization': get_token(subscription_key)})\n\nresponse = req.json()\n\ni = 0\n\nif len(response) == 0:\n    print(\"No jobs submitted\")\nelse:\n    while i < len(response):\n        r = \"ID: \" + json.dumps(response[i][\"id\"]) \\\n        + \"\\nStatus: \" + json.dumps(response[i][\"status\"]) \\\n        + \"\\nURI: \" + json.dumps(response[i][\"recordingsUrl\"]) \\\n        + \"\\nStatus Message: \" + json.dumps(response[i][\"statusMessage\"]) \\\n        + \"\\nTranscription URL: \" + json.dumps(response[i][\"resultsUrls\"]) + '\\n\\n'\n        print(r)\n        i += 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Transcription Data\n\nThis script downloads the JSON response for a completed transcription, and will load each output (if stereo) into a pandas dataframe showing each word, the offset from the start of the audio file, the duration, and the speaker."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Replace <Transcription_ID> below with the ID of the relevant job\nt_id = \"/3f42c932-9b3a-4371-bedb-610327dfa7a3\"\n\nreq = requests.get(request_url + t_id, headers = {'Ocp-Apim-Subscription-Key': subscription_key, 'Authorization': get_token(subscription_key)})\n\n\n#transcription results \nif len(response[0][\"resultsUrls\"]) >1:\n        \n    # Channel 1 audio transcription\n    tr_0 = json.dumps(response[0][\"resultsUrls\"][\"channel_0\"]).strip('\"\"')\n    c_0 = requests.get(tr_0)\n    channel_0 = json.dumps(c_0.json(), indent=4)\n    \n    # Channel 2 audio transcription\n    tr_1 = json.dumps(response[0][\"resultsUrls\"][\"channel_1\"]).strip('\"\"')\n    c_1 = requests.get(tr_1)\n    channel_1 = json.dumps(c_1.json(), indent=4)\n    \n    # Channel_0 output\n    output_0 = json.loads(channel_0)\n    \n    segments = len(output_0['AudioFileResults'][0]['SegmentResults'])\n    \n    i = 0\n    while i < segments:\n        if i < 1:\n            df0 = pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words'])\n            \n            cols = ['Word','Offset','Duration']\n            df0 = df0[cols]\n        else:\n            df0 = df0.append(pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words']),ignore_index=True)\n            \n        i += 1\n    \n    df0['Speaker'] = 1\n    \n    # Channel_1 output\n    output_1 = json.loads(channel_1)\n\n    segments_1 = len(output_1['AudioFileResults'][0]['SegmentResults'])\n    \n    i = 0\n    while i < segments:\n        if i < 1:\n            df1 = pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words'])\n            \n            cols = ['Word','Offset','Duration']\n            df1 = df1[cols]\n        else:\n            df1 = df1.append(pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words']),ignore_index=True)\n            \n        i += 1\n    \n    df1['Speaker'] = 2\n    \n    \n    df = df0.append(df1,ignore_index=True)\n    \nelse:\n    \n    # Channel 1 audio transcription\n    tr_0 = json.dumps(response[0][\"resultsUrls\"][\"channel_0\"]).strip('\"\"')\n    c_0 = requests.get(tr_0) \n    \n    # Channel_0 output\n    output_0 = json.loads(channel_0)\n    \n    segments = len(output_0['AudioFileResults'][0]['SegmentResults'])\n    \n    print(segments)\n    i = 0\n    \n    while i < segments:\n        if i < 1:\n            df = pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words'])\n            \n            cols = ['Word','Offset','Duration']\n            df = df[cols]\n        else:\n            df = df.append(pd.io.json.json_normalize(output_0['AudioFileResults'][0]['SegmentResults'][i]['NBest'][0]['Words']),ignore_index=True)\n\n            \n        i += 1\n    df['Speaker'] = 1\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df['EndTime'] = df['Offset'] + df['Duration']\n\ncols = ['Word','Offset','Duration','EndTime', 'Speaker']\ndf = df[cols]\n\n\ndf.sort_values(by=['Offset'],inplace=True)\ndf.reset_index()\n\nprint(df)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}